{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human-Centrerd Data Science ([HCDS](https://www.mi.fu-berlin.de/en/inf/groups/hcc/teaching/Summer-Term-2022/course_human_centered_data_science.html)) - Summer Term 2022 - [HCC](https://www.mi.fu-berlin.de/en/inf/groups/hcc/index.html) | [Freie Universit√§t Berlin](https://www.fu-berlin.de/)\n",
    "***\n",
    "# A1 - WarumUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the exercises you will be reading and writing Python code in Jupyter notebooks. Let's unpack this a bit!\n",
    "\n",
    "üêç Python is a programming language that has gained considerable traction over the last years, in various contexts, including data science and the digital humanities. If you have never written any Python before, it would be useful for you to familiarize yourself with the language, its basic constructs and conventions. It is popular for its versatility and readability. Speaking of which‚Ä¶\n",
    "\n",
    "üìó Jupyter notebooks are hybrid documents that contain both code and markup. So it becomes easy to mix programming and documentation. What you are looking at now is a text cell written in the markup language Markdown, further below you see code cells written in the programming language Python (note the light grey background), which contain computable code! When viewing the notebooks in Jupyter, you can double-click on any text cell to see its source.\n",
    "\n",
    "In this exercise you will get a bit acquainted with Python and Jupyter, and get to know a few handy libraries for working with data. If you already have experience, then it's still a good recap!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Hello world \n",
    "\n",
    "Okay, enough words. Let's dive right into it and start with a classic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code cell can be executed (i.e., run) by clicking `Shift + Enter`. Check out some CheatSheets [[1](https://www.edureka.co/blog/wp-content/uploads/2018/10/Jupyter_Notebook_CheatSheet_Edureka.pdf)] or [[2](https://cheatography.com/weidadeyue/cheat-sheets/jupyter-notebook/pdf_bw/)]‚ùó\n",
    "\n",
    "Of course we can set variables and extend them. Feel free to change the message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = \"hello world\"\n",
    "hello = hello + \" how are you!\"\n",
    "hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our first variable `hello` we can perform some string tricks, for example, we could change the capitalization. Please note: This will not change the variable `hello` itself , but will just return a capitalized new string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *Now it's your turn! The pencil stands for a small hands-on activity!*\n",
    "\n",
    "Try some string manipulations yourself. To get some inspiration, have a look at the [string methods](https://docs.python.org/3/library/stdtypes.html?#string-methods) that Python has built-in. Read the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hands-on inspirations, check out the following!\n",
    "# hello.center(30,'#')\n",
    "# hello.count('o')\n",
    "# hello.find('w')\n",
    "# hello.swapcase()\n",
    "# hello.startswith('hello')\n",
    "# hello.endswith('hello')\n",
    "# hello.split(' ')\n",
    "# hello.rjust(40, '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Let's get some packages\n",
    "\n",
    "Python itself provides only limited methods for working with more complex data. One of the main reasons for Python's (and  Jupyter's) popularity is the wide availability of software packages that provide powerful means for preparing, processing, presenting, and probing data. Throughout the exercises you will get to know a few packages, some of them highly specific tools and others more general-purpose libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the powerful `pandas` package is loaded and will answer to its nickname `pd`.\n",
    "\n",
    "üêº [Pandas](https://pandas.pydata.org) really is a data analysis workhorse with the DataFrame data structure being one of its main muscles. You will learn to love it! With pandas you can do simple and sophisticated operations over small and sizable datasets.\n",
    "\n",
    "Let's create a little toy dataset to give you a sense of how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_data = pd.read_csv(\"page-edits-data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample data `edits_data` contains the page edits of the Wikipedia article `COVID-19_pandemic` from 2020 differentiated by `editor-type`.\n",
    "\n",
    "To check whether the DataFrame was created successfully, we can type the variable name `edits_data`, to display its content as an output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output generated by a code cell is printed right below it. In the case of a DataFrame we get a table. By convention, the rows are the data entries and the columns are the data dimensions. The first column on the left side is the index. You can also use `.info()` to get a summary of that dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do something with our newly created DataFrame. For example, we could get the largest amount of edits by editor-type `user` using the ```max``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_data[\"user\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*‚úèÔ∏è What would it take to get the highest value of anonymous users?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_data[\"anonymous\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the entry belonging to the biggest amount of anonymous user, one needs to `loc`ate it via its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_data.loc[edits_data.anonymous.idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To access a specific column you can use (1) the dot operator (as with `.anonymous`) or passing a string to the indexing operator (as with `[\"users\"]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate averages for each numeric column by selecting them first and then calculating the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_data[['user', 'anonymous', 'group-bot', 'name-bot']].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is so much more to discover, some of which you will do over the course of the exercise. The [DataFrame page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) in the pandas reference gives a complete (long) list of all methods provided by the data structure. \n",
    "\n",
    "If you want to do something specific, but do not know the particular method name, a well-formulated search query in your favorite seach engine can help. In particular, the discussions on Stack Overflow contain various helpful entries. Quite often it is the case that somebody else has had a similar problem that you're trying to solve. The key then is to precisely formulate your query. For this, it is good to understand the basic terminology of Python, pandas, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå† Let's reach for the stars ... \n",
    "\n",
    "... and plot some data. For data visualization we now just use the library `Matplotlib`. You can use any other library for plotting you are familiar with ([plotly](https://plotly.com/python/), [seaborn](https://seaborn.pydata.org/), [ggpot(2)](https://plotnine.readthedocs.io/en/stable/index.html), [altair](https://altair-viz.github.io/), [pygal](http://www.pygal.org/en/stable/)) or just use the one, which comes with pandas.\n",
    "\n",
    "We will visualize the ratio of editor types using different kinds of charts, first using `pandas` itself and then `matplotlib`. First, we have to prepare the data a bit. Second, we calculate the `sum()` per column, make a Dataframe `to_frame()` out of it (cause it returns a `Series`), and then drop the first three rows using `iloc[]`, cause we don't need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = edits_data.sum().to_frame(name=\"groups\").iloc[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some basic bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "ax = plot_data.plot.bar()\n",
    "\n",
    "ax.set_xlabel('Editor Types')\n",
    "ax.set_ylabel('Number of Edits')\n",
    "ax.set_title('Number of edits of article XY')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*‚úèÔ∏è Plot a horizontal bar chart.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*‚úèÔ∏è Now, plot a pie chart using your own color map!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÄ Data wrangling\n",
    "This part of the exercise shows you a few tricks for preparing data for visualization. You will see how data can be loaded, parsed, and examined. For this we will continue to work with the **Pandas** package, in particular with the DataFrame data structure, and get to know a few additional helpers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading \n",
    "\n",
    "The first step is to bring the data into the purview of your notebook. So regardless of data structure and format, you need to have access to the data set. We will briefly cover four common ways of loading data into your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter data directly\n",
    "\n",
    "The simplest way to add data to your notebook is to enter it verbatim into the notebook as we have seen with the capital cities in the first tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.DataFrame({\n",
    "  \"name\": [\"Athens\", \"Bratislava\", \"Copenhagen\", \"Dublin\"],\n",
    "  \"area\": [39, 367.6, 86.2, 115],\n",
    "  \"elevation\": [170, 152, 14, 20],\n",
    "  \"population\": [664046, 429564, 602481, 553165]\n",
    "  }\n",
    ")\n",
    "\n",
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *Add a column for years when you have visited or plan to visit these cities*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open a local file\n",
    "\n",
    "You might also want to open a local file. We can open a file using Python's built-in `Zipfile` method in a context manager, after which we can `read()` its contents into the variable `covid_json`. The file is automatically closed when the context manager block is left. In this case the data is in the JSON format, which we will need to parse. We'll get to this later. You can open all kinds of formats. Here we know that we are dealing with a JSON file because of its extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(\"owid-covid-data.zip\", 'r') as file:\n",
    "    covid_json = file.read('owid-covid-data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_json[:500] # this displays the first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data via a URL\n",
    "\n",
    "There are some methods that can directly load a dataset via a URL, i.e., a web address. For others you might have to retrieve the file first to continue parsing it. The `requests` package helps you to send HTTP requests and retrieve the responses. \n",
    "\n",
    "In the following, the news feed of Tagesschau is retrieved via an HTTP GET request. Note that the news feed is made available as an XML format; of course, you can retrieve all kinds of file formats using this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://www.tagesschau.de/xml/rss2/')\n",
    "tagesschau_xml = response.text\n",
    "tagesschau_xml[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *Find the news feed for another webpage and try to load it!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use an API\n",
    "\n",
    "Some web platforms require the use of an API (application programming interface) to get access to their data. An API is a structured way to request and retrieve data. Oftentimes it is just a specific way to format the URL. The wikipedia edits data set we used in the beginning of this exercise was retrieved by using the [Wikimedia API](https://wikimedia.org/api/rest_v1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'https://github.com/YOURUSERNAME',\n",
    "    'From': 'YOURMAIL'\n",
    "}\n",
    "\n",
    "# Edpoint for getting Edits per Page\n",
    "ep = 'https://wikimedia.org/api/rest_v1/metrics/edits/per-page/en.wikipedia/Amy_Coney_Barrett/all-editor-types/daily/20200101/20201030'\n",
    "\n",
    "call = requests.get(ep, headers=headers)\n",
    "response = call.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *Prepare an API request to get the edits of some page in the german wikipedia.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing \n",
    "\n",
    "Apart from our little cities example, so far we have only loaded the data into unstructured strings. To be able to analyze the data, we have to turn the unstructured strings of symbols into a practical data structure of the DataFrame that we can work with. This process is commonly referred to as ‚Äòparsing‚Äô. \n",
    "\n",
    "As we have seen above, data can come in various file formats, which are in turn more or less appropriate for particular data structures. We'll cover four typical ones in the following section, but we will see more over the course of the tutorials to come.\n",
    "\n",
    "The different ways of loading data (e.g., by file path or URL) are independent from the particular data formats provided. For example, you can load CSV data from a local file or from a web address. While the files typically indicate with the extension what format they have, URLs or APIs may not have these. If it is not clear, you may have to check the documentation or take a peek into the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV\n",
    "\n",
    "The CSV format is probably the most common file format in the context of data analysis and visualization. CSV files contain tabular data that can be viewed and edited in a spreadsheet software such as Excel. CSV stands for [comma-separated values](https://en.wikipedia.org/wiki/Comma-separated_values), which seems to say it all: the data values are separated by commas and each row represents one item. However, there are also CSV files that use separators other than commas, such as tabs and semicolons. \n",
    "\n",
    "Let's load a CSV file! Thankfully Pandas has the convenient `read_csv()` method ready for us, which can open CSV data via a file path or URL, and turns it directly into a DataFrame object.\n",
    "\n",
    "A good source for CSV data is [Our world in Data](https://ourworldindata.org/), the datasets you find on [GitHub](https://github.com/owid/owid-datasets). As an exmple we will load the \"A Century of Work and Leisure\" dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_data = pd.read_csv(\"https://raw.githubusercontent.com/owid/owid-datasets/9e0249459233f8ef303eab7ed2d56ebbcf01e64c/datasets/A%20Century%20of%20Work%20and%20Leisure%20-%20Ramey%20and%20Francis%20(2009)/A%20Century%20of%20Work%20and%20Leisure%20-%20Ramey%20and%20Francis%20(2009).csv\")\n",
    "owid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *Try loading another CSV dataset from [GovData](https://www.govdata.de/)!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML\n",
    "\n",
    "[XML](https://en.wikipedia.org/wiki/XML) (Extensible Markup Language) is a data format, which can have very different kinds of hierarchical structures. XML files are common in a wide variety of contexts, including libraries, and especially in situations, in which the interoperability of multiple systems by several vendors needs to be ensured.\n",
    "\n",
    "The üå≤ **ElementTree** module will help us to parse the elements contained in an XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already retrieved the XML feed from Tagesschau (and saved it in the variable `tagesschau_xml`), we can now parse it directly from the string, i.e., using the method `ET.fromstring()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagesschau = ET.fromstring(tagesschau_xml)\n",
    "tagesschau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the root element of the XML feed (and all its children) in the variable `tagesschau`. \n",
    "\n",
    "Going through all items with `findall` and within these with `find` for specific sub-elements, we can extract the publication date and time and the title of the respective item. In the following these elements are put together into the DataFrame `tagesschau_df`. Note that it helps to peek into the XML source of the feed to know the specific element names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two empty lists\n",
    "dates = []\n",
    "titles = []\n",
    "\n",
    "# go through all item elements in the tree\n",
    "for item in tagesschau.findall('.//item'):\n",
    "  # extract date information and titles\n",
    "  dates.append( item.find('.//pubDate').text )\n",
    "  titles.append( item.find('.//title').text )\n",
    "\n",
    "# create a dataframe containing the two columns\n",
    "tagesschau_df = pd.DataFrame(    \n",
    "    {'date': dates,\n",
    "     'title': titles,\n",
    "    })  \n",
    "\n",
    "tagesschau_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *Each news item also contains a `description` element. Why not add a third column to the DataFrame?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *If you are curious you can also try out ü•£ **BeautifulSoup** to create the same DataFrame.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining\n",
    "\n",
    "You are now able to load and parse data from several formats. At this point, there are plenty of ways to inspect these datasets. We are going to try some simple methods to peek around the datasets. Once you have a tabular dataset ready as a DataFrame, there are quite a few convenient methods to view and explore its contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Head & tail\n",
    "\n",
    "You could start with looking at the beginning of the dataset with `head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *What do you think happens, when you replace `head()` with `tail()` ?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe & info\n",
    "\n",
    "You can also ask Pandas to provide some statistical descriptions (which are only applied to the columns containing numeric data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may not seem that useful to you yet. You may want to know what kind of datatypes the different columns contain and how many values are present. For this the `info()` method will be of help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this it is now possible to access specific columns by using their names.  But did you notice the long label for the first column? Let's rename the column `Retail & Recreation` into something short and sweet such as: `RR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_data = owid_data.rename(columns={\"Average weekly leisure estimates by age (Ramey and Francis (2009))\": \"leisure\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *Do you want to rename any other columns?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select & query\n",
    "We can select an individual column using single `[`square brackets`]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_data[\"leisure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚Ä¶¬†and we can select multiple columns using nested `[[`square brackets`]]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_data[[\"leisure\", \"Year\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *Which columns interest you? Replace `RR` and `Entity` with other column labels* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `leisure` column contains a lot of `NaN` - this stands for \"Not a Number\" and it means here that values are missing.\n",
    "\n",
    "In order to focus on the rows which do have missing data, we can squeeze in a requirement that we only want those rows, where the values in both above used columns are not missing, i.e., `notnull()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_data [owid_data[[\"leisure\", \"Year\"]].notnull().all(1) ] [[\"leisure\", \"Year\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *Formulate a query on another column:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four related methods for accessing rows, columns, and specific values, either by integer positons (iloc and iat) or by the labels (that is what is displayed in bold above).\n",
    "\n",
    "- `loc`: access rows and columns by label\n",
    "- `iloc`: access rows and columns by integer position \n",
    "- `at`: access a single value for a row/column label pair\n",
    "- `iat`: access a single value for a row/column pair by integer position \n",
    "\n",
    "For example, this way we can get the first entry in the `owid_data` DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_data.loc[0]\n",
    "\n",
    "# because the index here uses integers, iloc and loc do the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can also retrieve rows that match a query. With this we are retrieving the rows for women:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_data[owid_data['Entity'].str.contains('female')] [['Entity', \"Year\", \"Average weekly hours worked per person by demographic group (Ramey and Francis (2009))\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join some DataFrames. Lets look closer at the *anonymous* and *user* edits of the wikipedia article of `Amy_Coney_Barrett`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'https://github.com/YOURUSERNAME',\n",
    "    'From': 'YOURMAIL'\n",
    "}\n",
    "\n",
    "# Edpoint for getting Edits per Page\n",
    "ep1 = 'https://wikimedia.org/api/rest_v1/metrics/edits/per-page/en.wikipedia/Amy_Coney_Barrett/anonymous/daily/20200901/20201030'\n",
    "ep2 = 'https://wikimedia.org/api/rest_v1/metrics/edits/per-page/en.wikipedia/Amy_Coney_Barrett/user/daily/20200901/20201030'\n",
    "\n",
    "call1 = requests.get(ep1, headers=headers)\n",
    "response1 = call1.json()\n",
    "\n",
    "call2 = requests.get(ep2, headers=headers)\n",
    "response2 = call2.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we have the data, we need to extract the actual results and then `merge` the DataFrames. Check out what is meant by `left`, `right`, `inner` and `outer` -->  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_data = pd.json_normalize(response1, ['items', 'results'])\n",
    "user_data = pd.json_normalize(response2, ['items', 'results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(anon_data, user_data, on='timestamp', how='outer')\n",
    "merged_data = merged_data.rename(columns={\"edits_x\": \"anonymous edits\", \"edits_y\": \"user edits\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(merged_data['timestamp'], merged_data['user edits'], color=\"gray\")\n",
    "plt.plot(merged_data['timestamp'], merged_data['anonymous edits'], color=\"red\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend([\"user edits\", \"anonymous edits\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è Wikipedia Edits Plot\n",
    "‚úèÔ∏è *Now it's time for a challenge!*\n",
    "\n",
    "Try to visualize user edits and anonymous edits of two Wikipedia language versions of an article of your choice. We probably use more Wikipedia and wikidata during the next exercises, so it's worse getting familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è *Optional task using `covid_json`:*\n",
    "\n",
    "Try to visualize the positive cases per million over time for three countries of your choosing! There are several steps to it: First you need to convert the date column to a pandas date column using ```pd.to_datetime```, then you filter by the countries and finally you can visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Sources\n",
    "- [Pandas Tutorial: DataFrames in Python - DataCamp](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python)\n",
    "- [Introduction to Pandas\n",
    "](https://www.ritchieng.com/pandas-introduction/)\n",
    "- [The ElementTree XML API](https://docs.python.org/2/library/xml.etree.elementtree.html)\n",
    "- [Where do Mayors Come From? Querying Wikidata with Python and SPARQL - Towards Data Science](https://towardsdatascience.com/where-do-mayors-come-from-querying-wikidata-with-python-and-sparql-91f3c0af22e2)\n",
    "- [Loading data: Drive, Sheets, and Google Cloud Storage](https://colab.research.google.com/notebooks/io.ipynb) \n",
    "- [Examining Data Using Pandas | Linux Journal](https://www.linuxjournal.com/content/examining-data-using-pandas)\n",
    "- [Wikimedia API](https://wikimedia.org/api/rest_v1/#/)\n",
    "- [The 7 most popular ways to plot data in Python](https://opensource.com/article/20/4/plot-data-python)\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Credits\n",
    "\n",
    "This tutorial is adapted from the course [Data Visualization - Winter Term 2020/21](https://github.com/FUB-HCC/dataviz_course_2020) of HCC of Freie Universit√§t Berlin. Many thanks to [Dr. Christoph Kinkeldey](https://www.mi.fu-berlin.de/en/inf/groups/hcc/members/postdocs/kinkeldey.html) and [Tim korjakow](https://github.com/wittenator). Please note: Originally, the couse was adapted from [Information Visualization - Summer Term 2020](https://infovis.fh-potsdam.de/tutorials/) of Marian D√∂rk by [Fachhochschule Potsdam](https://www.fh-potsdam.de/).\n",
    "\n",
    "Same as the original inventors, we release the notebooks under the [Creative Commons Attribution license (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
