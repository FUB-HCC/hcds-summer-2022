{
 "cells": [
  {
   "cell_type": "raw",
   "id": "37f29fb1-ba1c-4092-882d-4199c27454e9",
   "metadata": {},
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ace0f54d-8a19-4d03-8de1-052a8c54c0cd",
   "metadata": {},
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670f3ef0-6ef7-4eee-8b09-9b92611643c6",
   "metadata": {},
   "source": [
    "# Task 4. Choose one of the provided common datasets to work with. (Adult Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fad84a-9ad6-4d78-b849-53d2de87c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.insert(1, \"../\")  \n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616de55a-3a3a-453a-ad5b-cd80c1a8123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = AdultDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd880b86-eb62-4722-bde1-3ef5207fc4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63a5ff-20a9-44bb-ac3e-201513138371",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.feature_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61abe951-89ec-4925-aaba-ab95f616f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.protected_attribute_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a72ce2-f4f7-4c0d-b7bf-32815c6d4e09",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load dataset, specifying protected attribute, and split dataset into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600368c-e2d0-45fd-abb4-c7247a9d129d",
   "metadata": {},
   "source": [
    "# 5. Select a protected attribute from your chosen dataset for the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2fb71-fe40-4b3a-bb3c-a47d3b5c8ce6",
   "metadata": {},
   "source": [
    "We are using \"Gender\" as a protected Attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c162938-6984-4386-a56e-13a33b32691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig = AdultDataset(\n",
    "    protected_attribute_names = ['sex'],\n",
    "    privileged_classes = [['Male']],\n",
    "    #protected_attribute_names=[\"race\", \"sex\"],           # this dataset also contains protected\n",
    "                                                 # attribute for \"sex\" which we do not\n",
    "                                                 # consider in this evaluation\n",
    "    #privileged_classes=[\"White\", \"Male\"],      # age >=25 is considered privileged\n",
    "    features_to_drop=['native-country', 'race', 'education', 'relationship', 'marital-status']\n",
    "    #features_to_keep=['race', 'capital-loss', 'education', 'education-num', 'workclass', 'age', 'fnlwgt', 'occupation',\n",
    "                      #'capital-gain', 'hours-per-week'] # ignore sex-related attributes\n",
    ")\n",
    "print(dataset_orig.feature_names)\n",
    "\n",
    "\n",
    "#privileged_groups = [{'race': 1, \"sex\": 1}]\n",
    "#unprivileged_groups = [{\"race\": 0, \"sex\": 0}]\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b4fdc-9a5e-4174-a1e9-fa391a541c65",
   "metadata": {},
   "source": [
    "Compute fairness metric on original training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab102b-32a1-4ab3-9cb8-dff858cbdf95",
   "metadata": {},
   "source": [
    "# 6. Compute multiple fairness metrics on that attribute (minimum: 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65733ce2-cdd7-4f96-8188-625c60fd950f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e16e48-0168-4706-9688-5d0522dbf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "difference = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf59e156-57ee-4aa5-b673-c046421c588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_metrics = [\"mean_difference\", \"average_odds_diffence\", \"balance_accuracy\", \"disparate_impact_ratio\", \"equalized_odds\", \"statistical_parity\"]\n",
    "bias_mitigation_algorithms = [\"reweighing\", \"disparate_impact_remover\", \"adversarial debiasing\",\n",
    "                              \"calibrated_eq_odds_postprocessing\", \"reject_option_classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7afcc60-abe0-4e0b-bedb-38277b519f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"mean_difference\": {\"before_reweighing\": None,\n",
    "                               \"after_reweighing\": None},\n",
    "           \n",
    "           \"disparate_impact_ratio\": {\"before_reweighing\": None,\n",
    "                                      \"after_reweighing\": None,\n",
    "                                      \"before_disparate_impact_remover\": None,\n",
    "                                      \"after_disparate_impact_remover\": None,\n",
    "                                      \"before_reject_option_classification\": None,\n",
    "                                      \"after_reject_option_classification\": None,\n",
    "                                      \"before_calibrated_eq_odds_postprocessing\": None,\n",
    "                                      \"after_calibrated_eq_odds_postprocessing\": None,\n",
    "                                      \"no_adversarial_debiasing\": None,\n",
    "                                      \"adversarial_debiasing\": None},\n",
    "           \n",
    "           \"balanced_accuracy\": {\"before_calibrated_eq_odds_postprocessing\": None,\n",
    "                                \"after_calibrated_eq_odds_postprocessing\": None,\n",
    "                                 \n",
    "                                \"before_reject_option_classification\": None,\n",
    "                                \"after_reject_option_classification\": None,\n",
    "                                 \n",
    "                                \"no_adversarial_debiasing\": None,\n",
    "                                \"adversarial_debiasing\": None},\n",
    "           \n",
    "           \"average_odds_difference\": {\"before_reject_option_classification\": None,\n",
    "                                       \"after_reject_option_classification\": None,\n",
    "                                       \n",
    "                                       \"before_calibrated_eq_odds_postprocessing\": None,\n",
    "                                       \"after_calibrated_eq_odds_postprocessing\": None,\n",
    "                                       \n",
    "                                       \"no_adversarial_debiasing\": None,\n",
    "                                      \"adversarial_debiasing\": None},\n",
    "           \n",
    "           \"statistical_parity_difference\": {\"before_reject_option_classification\": None,\n",
    "                                             \"after_reject_option_classification\": None,\n",
    "                                             \n",
    "                                             \"before_calibrated_eq_odds_postprocessing\": None,\n",
    "                                             \"after_calibrated_eq_odds_postprocessing\": None},\n",
    "           \n",
    "           \"conditional_use_accuracy_equality\": {\"before_reject_option_classification\": None,\n",
    "                                  \"after_reject_option_classification\": None,\n",
    "                                                \"before_calibrated_eq_odds_postprocessing\": None,\n",
    "                                                 \"after_calibrated_eq_odds_postprocessing\": None}\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b77927-f989-4ef3-8c42-5bce7b6a661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in results.items():\n",
    "    for nes_key, nes_value in value.items():\n",
    "        if nes_key == \"before_reweighing\" or nes_key == \"after_reweighing\":\n",
    "            continue\n",
    "        results[key][nes_key] = {\"validation\": None, \"test\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1190f-43da-4e96-b779-7be8bd58739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"mean_difference\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c644554-520e-42f4-b595-79c8394f499f",
   "metadata": {},
   "source": [
    "##### i. Mean difference <br> ii. Disparate Ratio without using CLassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0806e4-5254-4277-be3e-b8c72304c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"mean_difference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02e90e-8405-4d68-adb5-353a81bb9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"mean_difference\"][\"before_reweighing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755e121-61e3-47c8-bce9-5cee5086be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_orig = BinaryLabelDatasetMetric(dataset_orig, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "#mean_diff[\"before_reweighing\"] = metric_orig_train.mean_difference()\n",
    "results[\"mean_difference\"][\"before_reweighing\"] = metric_orig.mean_difference()\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd58f8d-3d13-4c11-b046-f06abc361461",
   "metadata": {},
   "source": [
    "##### ii. Disparate Impact Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff8438-93c2-4c63-9ab4-5bdc8ff4dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"disparate_impact_ratio\"][\"before_reweighing\"] = metric_orig.disparate_impact()\n",
    "metric_orig.disparate_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc6c09-814a-4183-8f16-0f6457cd3a67",
   "metadata": {},
   "source": [
    "### Step 4 Mitigate bias by transforming the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e8734-f8a3-41c3-b615-72bbb6a8d6d3",
   "metadata": {},
   "source": [
    "# 7. Try to mitigate bias using one of the bias mitigation algorithms provided by the toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78a942f-9ddd-4d74-b1f4-76368dec58a8",
   "metadata": {},
   "source": [
    "### Bias Mitigation Algorithm # 1: Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b523d5a-b474-42bd-b27c-613c3053c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf= RW.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b91eb-78fb-4697-a89e-c26925540e8a",
   "metadata": {},
   "source": [
    "##### What types of bias mitigation algorithm are available?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98dbe2-a0d3-46c4-b87d-986e3b499224",
   "metadata": {},
   "source": [
    "Three types of Bias Mitigation strategies:\n",
    "<ul>\n",
    "    <li>Pre-processing</li>\n",
    "    <li>In-processing</li>\n",
    "    <li>Post-processing</li>\n",
    "</ul>\n",
    "There are variety of Bias mitigation algorithms are available for each strategy, every algorithm is used to remove bias for specific metrics.In this exercise, we will apply these algorithms to mitigate Bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665c152-d176-45f1-9512-d4a502332dd9",
   "metadata": {},
   "source": [
    "##### Do you see a difference between the different types of algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f257ce4-0f1e-44df-9aa8-90c4ac28c38d",
   "metadata": {},
   "source": [
    "Yes, some of these algorithms during pre-processing step (i.e. Reweighing), while some mitigate bias during processing and some after processing. They have a different usage and solve specific problems corresponding to different measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03482185-0254-4f28-8a36-81ded22a8db5",
   "metadata": {},
   "source": [
    "# 8. Compute your fairness metrics again after the mitigation step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f85d97-6dd7-4c8e-aeb2-51a1f63b3f26",
   "metadata": {},
   "source": [
    "##### i. Mean difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86b410-7697-4467-8ee7-30322b3bddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_transf = BinaryLabelDatasetMetric(dataset_transf, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "results[\"mean_difference\"][\"after_reweighing\"] = metric_transf.mean_difference()\n",
    "display(Markdown(\"#### Transformed training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43968fe2-dd99-42cc-b3b3-48d023855fb5",
   "metadata": {},
   "source": [
    "##### ii. Disparate Impact after Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82558b0-bcb6-4aff-9c0f-7db303c1fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"disparate_impact_ratio\"][\"after_reweighing\"] = metric_transf.disparate_impact()\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786764be-9955-427b-8406-540ee25148c6",
   "metadata": {},
   "source": [
    "##### <br> ii. Disparate Impact Ratio (with Disparate Impact Remover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc417b88-22a8-4a25-8b92-82d5a1f2c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff6c5d-454e-447a-ae7c-652e7c23ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig.favorable_label, dataset_orig.unfavorable_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b5a41-820b-4e2c-8434-e5ce97e7525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_orig_new = BinaryLabelDatasetMetric(dataset_orig, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "results[\"disparate_impact_ratio\"][\"before_disparate_impact_remover\"] = metric_orig_new.disparate_impact()\n",
    "metric_orig_new.disparate_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa432be-98b4-482b-a322-68a07b6c0d3c",
   "metadata": {},
   "source": [
    "### Bias Mitigation Algorithm # 2: Disparate Impact Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a79c5-d19a-4119-b730-0b21fa921037",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = AdultDataset(\n",
    "protected_attribute_names = ['sex'],\n",
    "privileged_classes = [['Male']], features_to_drop=['native-country', 'race', 'education', 'relationship', 'marital-status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8e6e0-06a3-4b5a-98ab-92a9eaa62100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c9fdbd-6e35-4980-aedb-8bcfa24594e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "DIs = []\n",
    "def disparate_impact_remover(ad):\n",
    "    scaler = MinMaxScaler(copy=False)\n",
    "    \n",
    "    test, train = ad.split([16281])\n",
    "    train.features = scaler.fit_transform(train.features)\n",
    "    test.features = scaler.fit_transform(test.features)\n",
    "    index = train.feature_names.index(\"sex\")\n",
    "    \n",
    "    for level in tqdm(np.linspace(0., 1., 11)):\n",
    "        di = DisparateImpactRemover(repair_level=level)\n",
    "        train_repd = di.fit_transform(train)\n",
    "        test_repd = di.fit_transform(test)\n",
    "        X_train = np.delete(train_repd.features, index, axis=1)\n",
    "        X_test = np.delete(test_repd.features, index, axis=1)\n",
    "        y_train = train_repd.labels.ravel()\n",
    "        lr = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "        lr.fit(X_train, y_train)\n",
    "        test_repd_pred = test_repd.copy()\n",
    "        test_repd_pred.labels = lr.predict(X_test)\n",
    "        cm = BinaryLabelDatasetMetric(test_repd_pred, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "        DIs.append(cm.disparate_impact())\n",
    "    return np.mean(DIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bef3fa-a8de-4d17-9bfe-c7e0fd078da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparate_impact_ratio = disparate_impact_remover(ad)\n",
    "results[\"disparate_impact_ratio\"][\"after_disparate_impact_remover\"] = disparate_impact_ratio\n",
    "disparate_impact_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd9d1d-3356-466b-b940-589371e425fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np.linspace(0, 1, 11), DIs, marker='o')\n",
    "plt.plot([0, 1], [1, 1], 'g')\n",
    "plt.plot([0, 1], [0.7, 0.7], 'r')\n",
    "plt.ylim([0.4, 1.2])\n",
    "plt.ylabel('Disparate Impact (DI)')\n",
    "plt.xlabel('repair level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976b9f6-d44e-43eb-b5d5-a53bfb45d387",
   "metadata": {},
   "source": [
    "##### ii. Disparate Impact Ratio (with other Bias Mitigation Algorithms) <br>  iii. Balanced Accuracy <br> iv. Equalized Odds <br> v. Average Odds Difference <br> v. Conditional Use Accuracy Equality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c306f-abea-4b01-9e86-de47f624bc25",
   "metadata": {},
   "source": [
    "We will compute these values simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee05ed6-92b1-4a80-aa02-f3e5916a488d",
   "metadata": {},
   "source": [
    "### Splitting the dataset into Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b1090-761f-4e02-a4cb-72cdfcd2a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.7], shuffle=True)\n",
    "dataset_orig_val, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c29ca-8a51-4292-badb-f6eef696a493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296c82e-0c39-45f2-acba-787408c4a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train classifier on original data\n",
    "def train_classifier(dataset_orig_train):\n",
    "    #Logistic Regression\n",
    "    global scale_orig\n",
    "    scale_orig = StandardScaler()\n",
    "    X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "    y_train = dataset_orig_train.labels.ravel()\n",
    "    w_train = dataset_orig_train.instance_weights.ravel()\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train, sample_weight=dataset_orig_train.instance_weights)\n",
    "    y_train_predict = lr.predict(X_train)\n",
    "    \n",
    "    # positive class index\n",
    "    pos_ind = np.where(lr.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "    dataset_orig_train_pred = dataset_orig_train.copy()\n",
    "    dataset_orig_train_pred.labels = y_train_predict\n",
    "    return lr, dataset_orig_train_pred, pos_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c2413-5cf7-42c5-9da3-d66b764da941",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, dataset_orig_train_pred, pos_ind = train_classifier(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b612d62-c978-46a1-abc1-d2e3a7fda7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain scores for original validation and test sets\n",
    "def calculate_scores(dataset_orig_val, dataset_orig_test, classifier, pos_ind):\n",
    "    dataset_orig_val_pred = dataset_orig_val.copy(deepcopy=True)\n",
    "    X_val = scale_orig.transform(dataset_orig_val_pred.features)\n",
    "    y_val = dataset_orig_val_pred.labels\n",
    "    dataset_orig_val_pred.scores = classifier.predict_proba(X_val)[:,pos_ind].reshape(-1,1)\n",
    "\n",
    "    dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "    X_test = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "    y_test = dataset_orig_test_pred.labels\n",
    "    dataset_orig_test_pred.scores = classifier.predict_proba(X_test)[:,pos_ind].reshape(-1,1)\n",
    "    return (dataset_orig_val_pred, dataset_orig_test_pred), (X_val, y_val), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957ce54-0ff1-4f45-b8db-acc9e39aaff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset_orig_val_pred, dataset_orig_test_pred), (X_val, y_val), (X_test, y_test) = calculate_scores(dataset_orig_val, dataset_orig_test, lr, pos_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ef56f-8440-487e-99a9-e411a4b09391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal classification threshold from the validation set\n",
    "def find_optimal_clasific_thresh(dataset_orig_val, dataset_orig_val_pred):\n",
    "    num_thresh = 100\n",
    "    ba_arr = np.zeros(num_thresh)\n",
    "    class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "    \n",
    "    for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "        fav_inds = dataset_orig_val_pred.scores > class_thresh\n",
    "        #print(fav_inds[:5], dataset_orig_val_pred.scores[:5], class_thresh)\n",
    "        dataset_orig_val_pred.labels[fav_inds] = dataset_orig_val_pred.favorable_label\n",
    "        dataset_orig_val_pred.labels[~fav_inds] = dataset_orig_val_pred.unfavorable_label\n",
    "        #print(dataset_orig_val_pred.labels)\n",
    "        \n",
    "        classified_metric_orig_val = ClassificationMetric(dataset_orig_val, dataset_orig_val_pred, unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "        \n",
    "        ba_arr[idx] = 0.5*(classified_metric_orig_val.true_positive_rate() + classified_metric_orig_val.true_negative_rate())\n",
    "    \n",
    "    best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "    best_class_thresh = class_thresh_arr[best_ind]\n",
    "    print(\"Best balanced accuracy (no reweighing) = %.4f\" % np.max(ba_arr))\n",
    "    print(\"Optimal classification threshold (no reweighing) = %.4f\" % best_class_thresh)\n",
    "    return best_class_thresh, class_thresh_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc0536-5761-4f0a-a7b3-6e55d6deb7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_class_thresh, class_thresh_arr = find_optimal_clasific_thresh(dataset_orig_val, dataset_orig_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af21f5-15e7-43e5-a5d0-1037fbee43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_class_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f02663-a8b8-4b2d-acb5-1248ae9d58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_new = dataset_orig_train.copy(deepcopy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9653645-5cb3-4678-a57e-e8f0120cdd55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fa64e8e-4ec0-4a89-9fc1-1ffa635f57eb",
   "metadata": {},
   "source": [
    "### Bias Mitigation Algorithm # 3: Reject Option Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5186f45-29d4-4a15-b93b-0ee4831dd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  metric_name=\"Statistical parity difference\",\n",
    "                                  metric_ub=0.05, metric_lb=-0.05)\n",
    "ROC = ROC.fit(dataset_orig_val, dataset_orig_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cada3d-3322-4137-ab72-cc7c25ebedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e8bc7-5c36-45c9-9042-aa4cc8de2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import compute_metrics\n",
    "def compute_results(data, data_pred, clf, results, key, set_name = \"validation\"):\n",
    "    \"\"\"\n",
    "    set_name-> validation or test only.\n",
    "    clf-> classifier name\n",
    "    \"\"\"\n",
    "    # Metrics for the test set\n",
    "    fav_inds = data_pred.scores > best_class_thresh\n",
    "    data_pred.labels[fav_inds] = data_pred.favorable_label\n",
    "    data_pred.labels[~fav_inds] = data_pred.unfavorable_label\n",
    "    \n",
    "    display(Markdown(\"#### \" + str(set_name) + \" set\"))\n",
    "    display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "    metric_valid_bef = compute_metrics(data, data_pred, \n",
    "                unprivileged_groups, privileged_groups)\n",
    "\n",
    "    results[\"balanced_accuracy\"][\"before_\" + str(key)][set_name] = metric_valid_bef[\"Balanced accuracy\"]\n",
    "    results[\"disparate_impact_ratio\"][\"before_\" + str(key)][set_name] = metric_valid_bef[\"Disparate impact\"]\n",
    "    results[\"average_odds_difference\"][\"before_\" + str(key)][set_name] = metric_valid_bef[\"Average odds difference\"]\n",
    "    results[\"conditional_use_accuracy_equality\"][\"before_\" + str(key)][set_name] = metric_valid_bef[\"Conditional Use Accuracy Equality\"]\n",
    "    results[\"statistical_parity_difference\"][\"before_\" + str(key)][set_name] = metric_valid_bef[\"Statistical parity difference\"]\n",
    "    \n",
    "    # Transform the validation set\n",
    "    data_trans_pred = clf.predict(data_pred)\n",
    "    \n",
    "    display(Markdown(\"#### \" + str(set_name) + \" set\"))\n",
    "    display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "    metric_val_aft = compute_metrics(data, data_trans_pred, unprivileged_groups, privileged_groups)\n",
    "    \n",
    "    results[\"balanced_accuracy\"][\"after_\" + str(key)][set_name] = metric_val_aft[\"Balanced accuracy\"]\n",
    "    results[\"disparate_impact_ratio\"][\"after_\" + str(key)][set_name] = metric_val_aft[\"Disparate impact\"]\n",
    "    results[\"average_odds_difference\"][\"after_\" + str(key)][set_name] = metric_val_aft[\"Average odds difference\"]\n",
    "    results[\"conditional_use_accuracy_equality\"][\"after_\" + str(key)][set_name] = metric_val_aft[\"Conditional Use Accuracy Equality\"]\n",
    "    results[\"statistical_parity_difference\"][\"after_\" + str(key)][set_name] = metric_val_aft[\"Statistical parity difference\"]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba74c29-86f6-4528-8af2-a3c086c9af93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23a2bb-6f5b-4ae8-8dc3-bca8276b969d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6a20d-057c-4784-a6c2-4c3274ce5d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f46c9-92e7-4c56-93eb-fdfb3b124828",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"reject_option_classification\"\n",
    "results = compute_results(dataset_orig_test, dataset_orig_test_pred, ROC, results, key, \"test\")\n",
    "results = compute_results(dataset_orig_val, dataset_orig_val_pred, ROC, results, key, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc787647-db5d-494c-8bc8-4c35cf7d8aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768b113-886a-44d1-b81f-4c809c4149c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8292ea-10e3-4334-b432-ff9cc72cfc8a",
   "metadata": {},
   "source": [
    "### Bias Mitigation Algorithm # 4: Calibrated Equilized Odds Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9daa64-94af-4dfd-8002-49b3070f83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_constraint = \"fnr\" # \"fnr\", \"fpr\", \"weighted\"\n",
    "randseed = 5\n",
    "key = \"calibrated_eq_odds_postprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e383d41-582c-4f4d-a906-fc41e6830985",
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4233d6-8109-47a0-a26a-5a21fbf4520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odds equalizing post-processing algorithm\n",
    "# Learn parameters to equalize odds and apply to create a new dataset\n",
    "\n",
    "ceop = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     cost_constraint=cost_constraint,\n",
    "                                     seed=randseed)\n",
    "ceop = ceop.fit(dataset_orig_val, dataset_orig_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51203756-4662-40f0-8bf9-683bc1435bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compute_results(dataset_orig_val, dataset_orig_val_pred, ceop, results, key, \"validation\")\n",
    "results = compute_results(dataset_orig_test, dataset_orig_test_pred, ceop, results, key, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad87287-a9f8-4972-b502-fd7d6c8dcd21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d39b78b-e9aa-4ff4-88f6-3ad3d04f9b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed715464-2dcf-460d-8570-21706869cc7f",
   "metadata": {},
   "source": [
    "### Bias Mitigation Algorithm # 5: Adversarial Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b0b15-bf33-4188-bba9-29f66d99432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7856d4-1e02-49b9-aa56-99250d98f4b0",
   "metadata": {},
   "source": [
    "### Learn plain classifier without debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4e091-cca6-46a0-8f7b-e1ee54527b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='plain_classifier',\n",
    "                          debias=False,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994f3db-5185-4cb4-83fa-6b872ece2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d4bbd6-f368-4fdd-8434-4f1ea1d7bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
    "dataset_nodebiasing_val = plain_model.predict(dataset_orig_val)\n",
    "dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2d784-bd84-467d-8de4-604bdf4b7b38",
   "metadata": {},
   "source": [
    "##### Validation and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c98b37-006b-4c00-aaf4-7f5e2e7b8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Validation Set\n",
    "metric_val_bef = compute_metrics(dataset_orig_val, dataset_nodebiasing_val, unprivileged_groups, privileged_groups)\n",
    "\n",
    "results[\"balanced_accuracy\"][\"no_adversarial_debiasing\"][\"validation\"] = metric_val_bef[\"Balanced accuracy\"]\n",
    "results[\"disparate_impact_ratio\"][\"no_adversarial_debiasing\"][\"validation\"] = metric_val_bef[\"Disparate impact\"]\n",
    "results[\"average_odds_difference\"][\"no_adversarial_debiasing\"][\"validation\"] = metric_val_bef[\"Average odds difference\"]\n",
    "\n",
    "#For Test Set\n",
    "metric_test_bef = compute_metrics(dataset_orig_test, dataset_nodebiasing_test, unprivileged_groups, privileged_groups)\n",
    "\n",
    "results[\"balanced_accuracy\"][\"no_adversarial_debiasing\"][\"test\"] = metric_test_bef[\"Balanced accuracy\"]\n",
    "results[\"disparate_impact_ratio\"][\"no_adversarial_debiasing\"][\"test\"] = metric_test_bef[\"Disparate impact\"]\n",
    "results[\"average_odds_difference\"][\"no_adversarial_debiasing\"][\"test\"] = metric_test_bef[\"Average odds difference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38cda87-83d3-49e9-b75e-d1edca5200b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"disparate_impact_ratio\"][\"no_adversarial_debiasing\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1abcd7-497f-46d5-9d62-440c106bbfb8",
   "metadata": {},
   "source": [
    "### Apply in-processing algorithm Adversarial Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b3291-6fee-44b6-9fb8-72853a52c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1416e-6811-43c0-9692-c606d5ab02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c254e5e-1179-43cf-8d4c-a3377ee84353",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "dataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\n",
    "dataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f9c19-51ca-4974-a1b2-4646ead9513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "# Learn parameters with debias set to True\n",
    "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efa30b-e44c-46f3-975d-d40fe968ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "debiased_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b41b48-d589-4299-a648-efe194255a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bff60c-2a10-462c-b1f0-5fa9f6e9d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_val = debiased_model.predict(dataset_orig_val)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029ad0e-6a28-48d5-afb0-73c2bee08785",
   "metadata": {},
   "source": [
    "##### Validation and Test Data after implementing Adversarial Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3619e1-1233-4d8b-8d95-9bceeb9b24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Transformed Validation Set\n",
    "metric_transf_val_bef = compute_metrics(dataset_orig_val, dataset_debiasing_val, unprivileged_groups, privileged_groups)\n",
    "\n",
    "results[\"balanced_accuracy\"][\"adversarial_debiasing\"][\"validation\"] = metric_transf_val_bef[\"Balanced accuracy\"]\n",
    "results[\"disparate_impact_ratio\"][\"adversarial_debiasing\"][\"validation\"] = metric_transf_val_bef[\"Disparate impact\"]\n",
    "results[\"average_odds_difference\"][\"adversarial_debiasing\"][\"validation\"] = metric_transf_val_bef[\"Average odds difference\"]\n",
    "\n",
    "#For Transformed Test Set\n",
    "metric_transf_test_bef = compute_metrics(dataset_orig_test, dataset_debiasing_test, unprivileged_groups, privileged_groups)\n",
    "\n",
    "results[\"balanced_accuracy\"][\"adversarial_debiasing\"][\"test\"] = metric_transf_test_bef[\"Balanced accuracy\"]\n",
    "results[\"disparate_impact_ratio\"][\"adversarial_debiasing\"][\"test\"] = metric_transf_test_bef[\"Disparate impact\"]\n",
    "results[\"average_odds_difference\"][\"adversarial_debiasing\"][\"test\"] = metric_transf_test_bef[\"Average odds difference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a76ba4-d1ca-48f8-9cd0-d39759023561",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5885a836-98f8-49e8-974b-cec400183e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"mean_difference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0223a-6a72-455c-a573-b035fda12561",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"statistical_parity_difference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af9816-01d6-4b3f-b272-9eebf267819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4fa63-caeb-43da-92da-2eafb157d147",
   "metadata": {},
   "source": [
    "### 9. Compare your pre- and post mitigation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111b63d-4461-477b-a525-8a2ed5ac9321",
   "metadata": {},
   "source": [
    "#### Fairness Metric 1: Mean Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdad294-5bac-42f1-b968-64fbbc9e2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = pd.DataFrame(results[\"mean_difference\"], index = [\"Mean Difference\"])\n",
    "header = [[\"Reweighing (Pre-processing)\", \"Reweighing (Pre-processing)\"],\n",
    "          [\"before\", \"after\"]]\n",
    "mean_diff.columns = header\n",
    "mean_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542725d9-784e-43a5-bf7d-d47cdff62fda",
   "metadata": {},
   "source": [
    "As we can see that <b>Reweighing</b> (Bias Mitigation Algorithm) works well on the <b> Mean Difference</b> Fairness metric. It reduces it exponentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb271053-20d6-4b59-b890-a926988f235c",
   "metadata": {},
   "source": [
    "#### Fairness Metric 2: Disparate Impact Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71cdca8-8eb2-4051-a69b-2bbec07eb2b8",
   "metadata": {},
   "source": [
    "#### Comparision of Bias Mitigation Algorithms:\n",
    "<ol>\n",
    "    <li>Reweighing <b>(Pre-Processing)</b></li>\n",
    "    <li>Disparate Impact Remover <b>(Pre-Processing)</b></li>\n",
    "    <li>Reject Option Classifier <b>(Post-Processing)</b></li>\n",
    "    <li>Calibrated Equalised Odds <b>(Post-Processing)</b></li>\n",
    "    <li>Adversarial Debaising <b>(In-Processing)</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14466acc-1e8a-455e-8139-f6dba39b4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparate_impact_ratio = pd.DataFrame(results[\"disparate_impact_ratio\"])\n",
    "header = [[\"Pre-Processing\", \"Pre-Processing\", \"Pre-Processing\", \"Pre-Processing\", \"Post-Processing\", \"Post-Processing\",\n",
    "           \"Post-Processing\", \"Post-Processing\", \"In-Processing\", \"In-Processing\"], \n",
    "          [\"Reweighing\", \"Reweighing\", \"Disparate Impact Remover\", \"Disparate Impact Remover\",\"Reject Option Classifier\",\n",
    "           \"Reject Option Classifier\", \"Calibrated Equalised Odds\", \"Calibrated Equalised Odds\", \"Adversarial Debaising\", \"Adversarial Debaising\"],\n",
    "          [\"Before\", \"After\", \"Before\", \"After\", \"Before\", \"After\", \"Before\", \"After\", \"No Effect\", \"After Effect\"]]\n",
    "disparate_impact_ratio.columns = header\n",
    "disparate_impact_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf64d4-4630-458e-a7b9-5fe6430461ee",
   "metadata": {},
   "source": [
    "When it comes to <b>Disparate Impact Ratio</b> Fiarness metric, <b>Reweighing</b> outperforms other algorithms to increase the disparate impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e5521-08c5-4be6-9c1f-d835797c8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565a853-4ac5-4909-bb7e-dd1eaa44389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"balanced_accuracy\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ff0dc-24c3-462d-86a8-7b722fbbe301",
   "metadata": {},
   "source": [
    "#### Fairness Metric 3: Balanced Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e23e028-54e8-41e1-bff8-f8b9f08bccea",
   "metadata": {},
   "source": [
    "#### Comparision of Bias Mitigation Algorithms:\n",
    "<ol>\n",
    "    <li>Reject Option Classifier <b>(Post-Processing)</b></li>\n",
    "    <li>Calibrated Equalised Odds <b>(Post-Processing)</b></li>\n",
    "    <li>Adversarial Debaising <b>(In-Processing)</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1192db91-55cc-43f2-9ee8-1163bf601e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy = pd.DataFrame(results[\"balanced_accuracy\"])\n",
    "header = [[\"Post-Processing\", \"Post-Processing\",\n",
    "           \"Post-Processing\", \"Post-Processing\", \"In-Processing\", \"In-Processing\"], \n",
    "          [\"Reject Option Classifier\",\"Reject Option Classifier\", \"Calibrated Equalised Odds\", \"Calibrated Equalised Odds\",\n",
    "           \"Adversarial Debaising\", \"Adversarial Debaising\"],\n",
    "          [\"Before\", \"After\", \"Before\", \"After\", \"No Effect\", \"After Effect\"]]\n",
    "balanced_accuracy.columns = header\n",
    "balanced_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35a584-5d64-4c09-818d-a1e91560e4c8",
   "metadata": {},
   "source": [
    "<b>Calibrated Equalised Odds</b> Algorithm performs better than other two algorithms in the <b>Balanced Accuracy</b> metric. The balanced accuracy dropped slightly from <b>77%</b> to just <b>74%</b> in validation set whereas <b>76</b> to <b>73%</b> in the test set while using <b>Calibrated Equalised Odds</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c878c7-82ff-4b0e-953f-a0bde29b0db4",
   "metadata": {},
   "source": [
    "#### Fairness Metric 4: Average Odds Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f1fe30-dd64-4cc2-9d56-1d01d2a3db84",
   "metadata": {},
   "source": [
    "#### Comparision of Bias Mitigation Algorithms:\n",
    "<ol>\n",
    "    <li>Reject Option Classifier <b>(Post-Processing)</b></li>\n",
    "    <li>Calibrated Equalised Odds <b>(Post-Processing)</b></li>\n",
    "    <li>Adversarial Debaising <b>(In-Processing)</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c75495f-601d-4f10-8a73-e503dee774f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_odds_diff = pd.DataFrame(results[\"average_odds_difference\"])\n",
    "header = [[\"Post-Processing\", \"Post-Processing\",\n",
    "           \"Post-Processing\", \"Post-Processing\", \"In-Processing\", \"In-Processing\"], \n",
    "          [\"Reject Option Classifier\",\"Reject Option Classifier\", \"Calibrated Equalised Odds\", \"Calibrated Equalised Odds\",\n",
    "           \"Adversarial Debaising\", \"Adversarial Debaising\"],\n",
    "          [\"Before\", \"After\", \"Before\", \"After\", \"No Effect\", \"After Effect\"]]\n",
    "average_odds_diff.columns = header\n",
    "average_odds_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012afbc-e736-4c35-97c9-00d19276b15a",
   "metadata": {},
   "source": [
    "We got the lowest <b>Average Odds Difference (Equalised)</b> Fairness metric when using <b>Adversarial Debaising</b>/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d0743-744c-4b44-a9ce-ad7ea1ece420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48fec063-824a-49f2-95c4-c4ebd7661515",
   "metadata": {},
   "source": [
    "#### Fairness Metric 5: Statistical Parity Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c56b786-74a1-43b8-bfe0-ccbc1baf99f3",
   "metadata": {},
   "source": [
    "#### Comparision of Bias Mitigation Algorithms:\n",
    "<ol>\n",
    "    <li>Reject Option Classifier <b>(Post-Processing)</b></li>\n",
    "    <li>Calibrated Equalised Odds <b>(Post-Processing)</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db40509-15eb-4d55-b064-a16c4f2bb154",
   "metadata": {},
   "source": [
    "#### Reject Option Classification vs Calibrated Equalised Odds Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1925c78-cadd-4583-8e39-fb88c622b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spd = pd.DataFrame(results[\"statistical_parity_difference\"])\n",
    "header = [[\"Reject Option Classification\", \"Reject Option Classification\", \"Calibrated Equalised Odds Difference\", \"Calibrated Equalised Odds Difference\"],\n",
    "          [\"before_using\", \"after_using\", \"before_using\", \"after_using\"]]\n",
    "spd.columns = header\n",
    "spd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c678a-d43a-4aa2-809a-3a458e505f97",
   "metadata": {},
   "source": [
    "It can be seen that if we take the absolute differences <b>Calibrated Equalised Odds Difference</b> performs better than <b>Reject Option Classification</b> to reduce the bias between privilleged and unprivilleged groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f417f-454d-4e05-9c9c-5b0817a8c750",
   "metadata": {},
   "source": [
    "#### Fairness Metric 5: Conditional Use Accuracy Equality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780604fc-85f7-4bdd-be74-9a45cfb453a5",
   "metadata": {},
   "source": [
    "#### Comparision of Bias Mitigation Algorithms:\n",
    "<ol>\n",
    "    <li>Reject Option Classifier <b>(Post-Processing)</b></li>\n",
    "    <li>Calibrated Equalised Odds <b>(Post-Processing)</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036607f4-52cc-4433-a593-88dcaf4bbcbb",
   "metadata": {},
   "source": [
    "#### Reject Option Classification vs Calibrated Equalised Odds Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c8168b-e485-48fc-9466-a70684e7c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_use_acc_eq = pd.DataFrame(results[\"conditional_use_accuracy_equality\"])\n",
    "header = [[\"Post-Processing\", \"Post-Processing\",\n",
    "           \"Post-Processing\", \"Post-Processing\"], \n",
    "          [\"Reject Option Classification\",\"Reject Option Classification\", \"Calibrated Equalised Odds\", \"Calibrated Equalised Odds\"],\n",
    "          [\"Before\", \"After\", \"Before\", \"After\"]]\n",
    "cond_use_acc_eq.columns = header\n",
    "cond_use_acc_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80999a8-08ce-45cb-aab1-3b8167c6e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"conditional_use_accuracy_equality\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b53f81-7ba9-4851-ab4d-e6d39b75d22b",
   "metadata": {},
   "source": [
    "We got the best accuracy when using <b>Reject Option Classificatio</b>, there is just a slight decrease in <b>Conditional Use Accuracy Equality</b>. Wehreas in <b>Calibrated Equalised Odds</b>, this accuracy decreased significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff3525-b503-4bac-9c4c-9aff91bf6415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a6a4f7f-1711-41bc-bea3-4978aaaaf793",
   "metadata": {},
   "source": [
    "### <b> Repeat steps 6 to 9 again for at least four different bias mitigation algorithms. </b> After that, you can also look into a different protected attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a422b76-721e-413c-a662-d2d6f97e4df7",
   "metadata": {},
   "source": [
    "### Fairness Tools Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6cd96f-e22b-449e-bf21-fcbd8a2fa7b9",
   "metadata": {},
   "source": [
    "Do a research on other fairness tools that are currently available. What are their use cases? Have some of them been used in the development of commercial products? Take a closer look on at least three other tools.\n",
    "\n",
    "Document your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0795d21-ad49-4581-81c0-e89351c0699f",
   "metadata": {},
   "source": [
    "#### AI Fairness Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05884203-debf-4baf-b6a7-96dfd4b9303c",
   "metadata": {},
   "source": [
    "##### Aequitas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc194cb-a1e5-4daf-8abc-0b46f51e0c84",
   "metadata": {},
   "source": [
    "<b>Aequitas</b> is an open source bias and fairness audit toolkit that was released in 2018. It is designed to enable developers to seamlessly test models for a series of bias and fairness metrics in relation to multiple population sub-groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ff268-190c-47cf-98af-1332eb2569da",
   "metadata": {},
   "source": [
    "##### Microsoft Fair Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76db19-fcfe-4782-b6e5-53616fc88a27",
   "metadata": {},
   "source": [
    "As part of <b>Microsoft Fair Learn</b>, this is a general-purpose methodology for approaching fairness. Using binary classification, the method applies constraints to reduce fair classification to a sequence of cost-sensitive classification problems. Whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6151bc7-56b4-41a1-b59e-af05daa17173",
   "metadata": {},
   "source": [
    "##### What-if Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae38da-66ee-45cb-a891-60cf2ee17841",
   "metadata": {},
   "source": [
    "The <b>What-if Tool</b> from Google is an open-source TensorBoard web application which lets users analyse an ML model without writing code. It visualises counterfactuals so that users can compare a data-point to the most similar point where the model predicts a different result. In addition, users can explore the effects of different classification thresholds, taking into account constraints such as different numerical fairness criteria.  There are a number of demos available – showing how the different functions work on pre-trained models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93445e-bd4b-42db-9e1e-6769f479d164",
   "metadata": {},
   "source": [
    "##### Deon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00765b6-5860-47b2-a60e-2e2bc598317b",
   "metadata": {},
   "source": [
    "An ethics checklist for responsible data science, <b>Deon</b> represents a starting point for teams to evaluate considerations related to advanced analytics and machine learning applications from data collection through deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef22473-fa6c-45d7-839c-0aad83ab2b92",
   "metadata": {},
   "source": [
    "##### Model Cards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec2b59-fc28-4ac8-bd19-b9b827f64bed",
   "metadata": {},
   "source": [
    "As machine learning applications are becoming increasingly ubiquitous, Google Research put forward the idea of <b>Model Cards</b> to confirm that the intent of a given model matches its use case. This credential is similar to the documentation of a car’s crash test rating in different environmental conditions. Model Cards can help stakeholders to <b>understand conditions under which the model is safe and appropriate to implement.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7e9ab-7757-428f-82f0-030e4cd038fc",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27435498-8870-4d9a-9f31-5311ff0d0743",
   "metadata": {},
   "source": [
    "All the Bias Mitigation Algorithms works well according to their usability. They are not supposed to satify for all metrics, their functionality is limited to their use case to solve or mitigate specific type of bias in data. In our Adult dataset, <b>Reweighing</b> performs better to reduce <b>mean_difference</b> and increase <b>disparate impact ratio</b>, while Adversarial Debaising works well to reduce <b>Average Odds Difference</b> between privilled and unprivilleged groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca566822-e072-4a3f-8a68-9cc7b2767603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
