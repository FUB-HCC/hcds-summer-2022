{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff19dd6",
   "metadata": {},
   "source": [
    "#  Assignment 4 - Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c685d",
   "metadata": {},
   "source": [
    "## Contributers:\n",
    "\n",
    "\n",
    "#### Reni Koci (renik98) \n",
    "#### Viktoria Voucheva (vouchev00)\n",
    "#### Muhammad Furqan Rafique (rafim96)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5a792",
   "metadata": {},
   "source": [
    "In this assignment we will be looking at the AI Fairness 360 toolkit. It is an \"extensible open-source library containing techniques developed by the research community to help detect and mitigate bias in machine learning models throughout the AI application lifecycle. AI Fairness 360 package includes multiple methods for computing different fairness metrics and algorithms for bias mitigation. In this task, you will get familiar with AIF360. You will use it to try to detect and mitigate bias on a self-chosen dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9498b",
   "metadata": {},
   "source": [
    "## <center>1. AIF360 Installation Instructions (Ubuntu) </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2144179",
   "metadata": {},
   "source": [
    "Execute following cmd's for setup \n",
    "\n",
    "1. conda deactivate (OR deactivate)\n",
    "2. sudo pip3 install virtualenv \n",
    "3. sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "4. sudo apt install python3.7\n",
    "5. sudo apt-get install python3.7-distutils\n",
    "6. virtualenv -p /usr/bin/python3.7 venv\n",
    "7. source venv/bin/activate\n",
    "8. pip3 install –upgrade pip\n",
    "9. pip install aif360\n",
    "\n",
    "\n",
    "10. python -m notebook        (Open the notebook using this cmd and start working)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0263586",
   "metadata": {},
   "source": [
    "## <center> 4 & 5. Choosing datasets and a protected attribute </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e908c6",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05f8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aif360.datasets import BankDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762ab63",
   "metadata": {},
   "source": [
    "We are selecting BankDataset. And we are using AGE (age >=25 is considered privileged) as protected attribute. This dataset also contains protected attribute like \"marital\" which we do not consider in this evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24eb272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 10641 rows removed from BankDataset.\n"
     ]
    }
   ],
   "source": [
    "dataset_orig = BankDataset(\n",
    "    protected_attribute_names=['age'],           \n",
    "    privileged_classes=[lambda x: x >= 25],       \n",
    "    features_to_drop=['marital']                 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d3d047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True) # split into train(70%)-test(30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae4d9295",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313337e",
   "metadata": {},
   "source": [
    "## <center> 6. Compute multiple fairness metrics  </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3e56f",
   "metadata": {},
   "source": [
    "#### Here, we'll use the metrics from BinaryLabelDatasetMetric class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19947a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import BinaryLabelDatasetMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c34bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________Metrics on Original training dataset__________\n",
      "Mean difference                             = 0.108291\n",
      "Base rate                                   = 0.127210\n",
      "Disparate impact                            = 1.872499\n",
      "smoothed_empirical_differential_fairness    = 0.629008\n"
     ]
    }
   ],
   "source": [
    "binary_metric = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"_________Metrics on Original training dataset__________\")\n",
    "print(\"Mean difference                             = %f\" % binary_metric.mean_difference())\n",
    "print(\"Base rate                                   = %f\" % binary_metric.base_rate())\n",
    "print(\"Disparate impact                            = %f\" % binary_metric.disparate_impact())\n",
    "print(\"smoothed_empirical_differential_fairness    = %f\" % binary_metric.smoothed_empirical_differential_fairness())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367facc",
   "metadata": {},
   "source": [
    "## <center> 7,8 & 9. Mitigate bias using bias mitigation algorithm plus recompute and compare the fairness metrics  </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0807dab",
   "metadata": {},
   "source": [
    "For this part we'll use different bias mitigation algorithms and will see their impact on fairness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a384941",
   "metadata": {},
   "source": [
    "### 1. Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "212c6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,privileged_groups=privileged_groups)\n",
    "RW_transf_train = RW.fit_transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c224b8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________Metrics on Reweighing Transform data__________\n",
      "Mean difference                             = -0.000000\n",
      "Base rate                                   = 0.127210\n",
      "Disparate impact                            = 1.000000\n",
      "smoothed_empirical_differential_fairness    = 0.004636\n"
     ]
    }
   ],
   "source": [
    "binary_metric = BinaryLabelDatasetMetric(RW_transf_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"_________Metrics on Reweighing Transform data__________\")\n",
    "print(\"Mean difference                             = %f\" % binary_metric.mean_difference())\n",
    "print(\"Base rate                                   = %f\" % binary_metric.base_rate())\n",
    "print(\"Disparate impact                            = %f\" % binary_metric.disparate_impact())\n",
    "print(\"smoothed_empirical_differential_fairness    = %f\" % binary_metric.smoothed_empirical_differential_fairness())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bae82f",
   "metadata": {},
   "source": [
    "### 2. Disparate Impact Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21478129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "dis_imp_rem =DisparateImpactRemover(repair_level=0.50, sensitive_attribute='age')\n",
    "dis_imp_rem_transf_train = dis_imp_rem.fit_transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15419dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________Metrics on Disparate Impact Remover Transform data__________\n",
      "Mean difference                             = 0.108291\n",
      "Base rate                                   = 0.127210\n",
      "Disparate impact                            = 1.872499\n",
      "smoothed_empirical_differential_fairness    = 0.629008\n"
     ]
    }
   ],
   "source": [
    "binary_metric = BinaryLabelDatasetMetric(dis_imp_rem_transf_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"_________Metrics on Disparate Impact Remover Transform data__________\")\n",
    "print(\"Mean difference                             = %f\" % binary_metric.mean_difference())\n",
    "print(\"Base rate                                   = %f\" % binary_metric.base_rate())\n",
    "print(\"Disparate impact                            = %f\" % binary_metric.disparate_impact())\n",
    "print(\"smoothed_empirical_differential_fairness    = %f\" % binary_metric.smoothed_empirical_differential_fairness())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db93229",
   "metadata": {},
   "source": [
    "### 3. LFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda24ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import LFR\n",
    "lfr =LFR(unprivileged_groups=unprivileged_groups,privileged_groups=privileged_groups)\n",
    "lfr_transf_train = lfr.fit_transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a25df9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________Metrics on LFR Transform data__________\n",
      "Mean difference                             = 0.000000\n",
      "Base rate                                   = 0.000000\n",
      "Disparate impact                            = nan\n",
      "smoothed_empirical_differential_fairness    = 3.524629\n"
     ]
    }
   ],
   "source": [
    "binary_metric = BinaryLabelDatasetMetric(lfr_transf_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"_________Metrics on LFR Transform data__________\")\n",
    "print(\"Mean difference                             = %f\" % binary_metric.mean_difference())\n",
    "print(\"Base rate                                   = %f\" % binary_metric.base_rate())\n",
    "print(\"Disparate impact                            = %f\" % binary_metric.disparate_impact())\n",
    "print(\"smoothed_empirical_differential_fairness    = %f\" % binary_metric.smoothed_empirical_differential_fairness())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7150a129",
   "metadata": {},
   "source": [
    "### 4. MetaFairClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b0edc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.inprocessing import MetaFairClassifier\n",
    "mfc = MetaFairClassifier(sensitive_attr='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13ade64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fairlearn # if you get error while running above commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4bbe22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mfc.fit(dataset_orig_train)\n",
    "mfc_transf_train = x.predict(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9da0309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________Metrics on MetaFairClassifier Transform data__________\n",
      "Mean difference                             = 0.342318\n",
      "Base rate                                   = 0.240296\n",
      "Disparate impact                            = 2.485021\n",
      "smoothed_empirical_differential_fairness    = 0.910017\n"
     ]
    }
   ],
   "source": [
    "binary_metric = BinaryLabelDatasetMetric(mfc_transf_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"_________Metrics on MetaFairClassifier Transform data__________\")\n",
    "print(\"Mean difference                             = %f\" % binary_metric.mean_difference())\n",
    "print(\"Base rate                                   = %f\" % binary_metric.base_rate())\n",
    "print(\"Disparate impact                            = %f\" % binary_metric.disparate_impact())\n",
    "print(\"smoothed_empirical_differential_fairness    = %f\" % binary_metric.smoothed_empirical_differential_fairness())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf3248",
   "metadata": {},
   "source": [
    "#### Types of bias mitigation algorithms:\n",
    "There are mainly three types of bias mitigation algorithms:\n",
    "1. pre-processing\n",
    "2. in-processing\n",
    "3. post-processing\n",
    "\n",
    "Pre-processing algorithms are used to mitigate the biasness before the model training. This category has the following algorithms:\n",
    "1. Reweighing: only changes weights applied to training samples; it does not change any feature or label values. Therefore, it may be a preferred option in case the application does not allow for value changes\n",
    "2. Disparate impact remover: yield modified datasets in the same space as the input training data\n",
    "3. LFR: the pre-processed dataset is in a latent space. \n",
    "\n",
    "Whereas, in in-processing algorithms, the training algorithms are modified to learn fairness while learning predictive model parameters. Examples are :\n",
    "1. AdversarialDebiasing: Learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary’s ability to determine the protected attribute from the predictions\n",
    "2. MetaFairClassifier: The meta-algorithm here takes the fairness metric as part of the input and returns a classifier optimized w.r.t. that fairness metric\n",
    "\n",
    "Examples of post-processing algorithms are :\n",
    "1. Calibrated equalized odds postprocessing: a post-processing technique that optimizes over calibrated classifier score outputs to find probabilities with which to change output labels with an equalized odds objective\n",
    "2. Equalized odds postprocessing: a post-processing technique that solves a linear program to find probabilities with which to change output labels to optimize equalized odds\n",
    "\n",
    "In this assignment we make use of 3 pre-process and 1 in-process bias mitigation algorithm, and our findings are below.\n",
    "\n",
    "#### Results and Comparison \n",
    "In our case, LFR is performing well whereas the in-process algorithm 'MetaFairClassifier' is giving the worst results. Further, we have also found out that each algorithm even within the same category is built for different purposes, and we can not just simply compare all algorithms or metrics as we can do in normal ML models. Moreover, AIF360 has no full support for the dataset that we have chosen for this assignment, otherwise, we would have also implemented post-processing or more in-processing algorithms.\n",
    "\n",
    "Reference :- https://github.com/Trusted-AI/AIF360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d658723",
   "metadata": {},
   "source": [
    "### Fairness Tools Overview:\n",
    "\n",
    "#### Aequitas\n",
    "Is an open source bias and fairness audit toolkit that was released in 2018. It is designed to enable developers to seamlessly test models for a series of bias and fairness metrics in relation to multiple population sub-groups.\n",
    "\n",
    "#### Microsoft Fair Learn\n",
    "As part of Microsoft Fair Learn, this is a general-purpose methodology for approaching fairness. Using binary classification, the method applies constraints to reduce fair classification to a sequence of cost-sensitive classification problems. Whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints.\n",
    "\n",
    "#### What-if Tool from Googl\n",
    "The What-if Tool from Google is an open-source TensorBoard web application which lets users analyse an ML model without writing code. It visualises counterfactuals so that users can compare a data-point to the most similar point where the model predicts a different result. In addition, users can explore the effects of different classification thresholds, taking into account constraints such as different numerical fairness criteria.  There are a number of demos available – showing how the different functions work on pre-trained models.\n",
    "\n",
    "Reference:- https://www.aiforpeople.org/tools-for-fairness/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
